# --- ðŸ“¦ Ð¡Ñ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ð° Ð±Ñ–Ð±Ð»Ñ–Ð¾Ñ‚ÐµÐºÐ° ---
import argparse
import csv
import os
import json
import random
from threading import Event

import numpy as np

# --- ðŸ“ Ð’Ð½ÑƒÑ‚Ñ€Ñ–ÑˆÐ½Ñ– Ð¼Ð¾Ð´ÑƒÐ»Ñ– ---
from config import (
    CACHE_CAPACITY, NUM_OBJECTS, NUM_REQUESTS, DELAY_T,
    CHECK_INTERVAL,
    LOGREG_MODEL_PATH,
    ALPHA, GAMMA, EPSILON_START, EPSILON_MIN, EPSILON_DECAY,
    ETA_START, ETA_MIN, ETA_DECAY_K,
    STATS_FILE,
    PARAMS_PATH, params
)

from domain.object import ObjectData
from generator.object_generator import ObjectGenerator
from generator.request_generator import RequestGenerator

from cache.in_memory_cache import InMemoryCache
from cache.classics import LRUCache, LFUCache

from utils.persistent import (
    save_objects, load_objects,
    save_q_table, load_q_table
)
from utils.init_helpers import init_logreg
from utils.shared_index import SharedIndex

from agents.q_learning_agent import QLearningAgent
from agents.logreg_agent import LogRegAgent

from buffers.q_buffer import QBuffer
from buffers.logreg_buffer import LogRegBuffer

from updater.q_updater import QUpdater
from updater.logreg_updater import LogRegUpdater

parser = argparse.ArgumentParser()
parser.add_argument("--reset", type=int, default=0)
args = parser.parse_args()

if args.reset:
    from utils.reset import reset_all
    reset_all()

random.seed(42)
np.random.seed(42)
print("[Seed] âœ… Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑ”Ñ‚ÑŒÑÑ Ñ„Ñ–ÐºÑÐ¾Ð²Ð°Ð½Ð¸Ð¹ random seed (Ñ€ÐµÐ¶Ð¸Ð¼ ÑÑ‚Ð°Ð±Ñ–Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ñ‚ÐµÑÑ‚ÑƒÐ²Ð°Ð½Ð½Ñ)")

# --- Ð¡Ñ‚Ð²Ð¾Ñ€ÐµÐ½Ð½Ñ Ð¿Ð¾Ñ‡Ð°Ñ‚ÐºÐ¾Ð²Ð¾Ð³Ð¾ CSV Ñ– Ð¼Ð¾Ð´ÐµÐ»Ñ–, ÑÐºÑ‰Ð¾ Ñ‚Ñ€ÐµÐ±Ð° ---
init_logreg()

# --- Ð†Ð½Ñ–Ñ†Ñ–Ð°Ð»Ñ–Ð·Ð°Ñ†Ñ–Ñ ---
int_cache = InMemoryCache(max_size=CACHE_CAPACITY)
lru_cache = LRUCache(max_size=CACHE_CAPACITY)
lfu_cache = LFUCache(max_size=CACHE_CAPACITY)

q_buffer = QBuffer(delay_T=DELAY_T)
logreg_buffer = LogRegBuffer(delay_T=DELAY_T)
q_table = load_q_table()
reload_flag = Event()

q_agent = QLearningAgent(
    num_actions=2,
    alpha=ALPHA,
    gamma=GAMMA,
    epsilon_start=EPSILON_START,
    epsilon_min=EPSILON_MIN,
    epsilon_decay=EPSILON_DECAY,
    q_table=q_table,
    buffer=q_buffer
)

logreg_agent = LogRegAgent(
    model_path=LOGREG_MODEL_PATH,
    buffer=logreg_buffer,
    cache=int_cache,
    reload_flag=reload_flag
)

obj_gen = ObjectGenerator()
all_objects = load_objects()
if not all_objects:
    all_objects = {obj.id: obj for obj in obj_gen.generate_batch(NUM_OBJECTS)}
    save_objects(all_objects)

req_gen = RequestGenerator(object_ids=list(all_objects.keys()))
requests = req_gen.generate_requests(NUM_REQUESTS)

shared_index = SharedIndex(0)
q_updater = QUpdater(
    buffer=q_buffer,
    q_table=q_table,
    requests=requests,
    check_interval=CHECK_INTERVAL,
    shared_index=shared_index
)

logreg_updater = LogRegUpdater(
    buffer=logreg_buffer,
    requests=requests,
    reload_flag=reload_flag,
    check_interval=CHECK_INTERVAL,
    shared_index=shared_index
)

q_updater.start()
logreg_updater.start()

int_hits = 0
lru_hits = 0
lfu_hits = 0

interval = 100

# --- ÐžÑÐ½Ð¾Ð²Ð½Ð¸Ð¹ Ñ†Ð¸ÐºÐ» ---
for i in range(len(requests)):
    req = requests[i]
    object_id = req["object_id"]
    obj = all_objects[object_id]

    shared_index.set(i)

    # --- ÐŸÑ–Ð´Ñ€Ð°Ñ…ÑƒÐ½Ð¾Ðº Ñ…Ñ–Ñ‚Ñ–Ð² ---
    if lru_cache.get(object_id) is not None:
        lru_hits += 1
    else:
        lru_cache.add(obj)
    if lfu_cache.get(object_id) is not None:
        lfu_hits += 1
    else:
        lfu_cache.add(obj)
    if int_cache.get(object_id) is None:
        action = q_agent.act(object_data=obj, current_index=i)
        if action == 1:
            if int_cache.is_full():
                to_evict = logreg_agent.act(current_index=i)
                if to_evict:
                    int_cache.delete(to_evict)
            int_cache.add(obj)
    else:
        int_hits += 1


print("[Main] â³ ÐžÑ‡Ñ–ÐºÑƒÐ²Ð°Ð½Ð½Ñ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð½Ñ Ñ€Ð¾Ð±Ð¾Ñ‚Ð¸ Ð°Ð¿Ð´ÐµÐ¹Ñ‚ÐµÑ€Ñ–Ð²...")

q_updater.stop()
logreg_updater.stop()
q_updater.join()
logreg_updater.join()

# --- Ð—Ð±ÐµÑ€ÐµÐ¶ÐµÐ½Ð½Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ–Ð² ---
save_q_table(q_table)

with open(STATS_FILE, "a", newline="") as f:
    writer = csv.writer(f)
    writer.writerow([int_hits, lru_hits, lfu_hits])

params["EPSILON_START"] = max(EPSILON_MIN, EPSILON_START * (1 - EPSILON_DECAY))
params["ETA_START"] = max(ETA_MIN, ETA_START * (1 - ETA_DECAY_K))

with open(PARAMS_PATH, "w") as f:
    json.dump(params, f, indent=2)

print(f"[Main] ðŸ” ÐžÐ½Ð¾Ð²Ð»ÐµÐ½Ð¾ Îµ={params['EPSILON_START']:.4f}, Î·={params['ETA_START']:.4f}")


# # -- Ð”Ð¾Ð´Ð°Ñ‚ÐºÐ¾Ð²Ð° Ñ–Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ñ–Ñ Ð¿Ñ€Ð¾ ÐºÐ¾Ð½Ñ„Ñ–Ð³ÑƒÑ€Ð°Ñ†Ñ–ÑŽ ---
# print("\n--- ÐšÐ¾Ð½Ñ„Ñ–Ð³ÑƒÑ€Ð°Ñ†Ñ–Ñ ÐµÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ñƒ ---")
# print(f"ÐšÑ–Ð»ÑŒÐºÑ–ÑÑ‚ÑŒ ÑƒÐ½Ñ–ÐºÐ°Ð»ÑŒÐ½Ð¸Ñ… Ð¾Ð±Ê¼Ñ”ÐºÑ‚Ñ–Ð²:   {NUM_OBJECTS}")
# print(f"ÐžÐ±Ê¼Ñ”Ð¼ ÐºÐµÑˆÑƒ:                      {CACHE_CAPACITY}")
# print(f"Ð—Ð°Ð³Ð°Ð»ÑŒÐ½Ð° ÐºÑ–Ð»ÑŒÐºÑ–ÑÑ‚ÑŒ Ð·Ð°Ð¿Ð¸Ñ‚Ñ–Ð²:      {NUM_REQUESTS}")
#
# # --- ÐŸÑ–Ð´ÑÑƒÐ¼ÐºÐ¸ ---
# print("\n--- Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¸ ---")
# print(f"ÐšÐµÑˆ-Ñ…Ñ–Ñ‚Ð¸ (Intellectual):        {int_hits}")
# print(f"ÐšÐµÑˆ-Ñ…Ñ–Ñ‚Ð¸ (LRU):                 {lru_hits}")
# print(f"ÐšÐµÑˆ-Ñ…Ñ–Ñ‚Ð¸ (LFU):                 {lfu_hits}")
#
# # --- ÐŸÐ¾Ñ€Ñ–Ð²Ð½ÑÐ½Ð½Ñ Ñƒ Ð²Ñ–Ð´ÑÐ¾Ñ‚ÐºÐ°Ñ… ---
# improvement_vs_lru = 100 * (int_hits - lru_hits) / lru_hits
# improvement_vs_lfu = 100 * (int_hits - lfu_hits) / lfu_hits
#
# print(f"\nÐŸÐ¾ÐºÑ€Ð°Ñ‰ÐµÐ½Ð½Ñ Ð¿Ð¾Ñ€Ñ–Ð²Ð½ÑÐ½Ð¾ Ð· LRU:     {improvement_vs_lru:.2f}%")
# print(f"ÐŸÐ¾ÐºÑ€Ð°Ñ‰ÐµÐ½Ð½Ñ Ð¿Ð¾Ñ€Ñ–Ð²Ð½ÑÐ½Ð¾ Ð· LFU:     {improvement_vs_lfu:.2f}%")
#
# if improvement_vs_lru > 2. or improvement_vs_lru > 2.:
#     plot_cache_hits({
#         "Intellectual (QLearn + LogReg)": smart_curve,
#         "LRU": lru_curve,
#         "LFU": lfu_curve
#     }, interval)